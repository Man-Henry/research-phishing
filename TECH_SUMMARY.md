# üìä T·ªïng H·ª£p C√¥ng Ngh·ªá & Thu·∫≠t To√°n
# Technologies & Algorithms Summary

---

## üéØ **1. MACHINE LEARNING ALGORITHMS**

### **Random Forest Classifier**
- **Email Detection**: 200 trees, max_depth=15
- **File Detection**: 200 trees, max_depth=10
- **Features**: 17 (email), 11 (file)
- **Accuracy**: 85-95%
- **Library**: scikit-learn

### **Gradient Boosting** (Optional)
- Alternative model cho training
- 100 estimators, learning_rate=0.1

### **StandardScaler**
- Feature normalization
- Z-score standardization: `(x - Œº) / œÉ`

---

## üî¢ **2. FEATURE EXTRACTION ALGORITHMS**

### **Email Features (17 dimensions)**
```
1. URL count
2. Has shortened URL (bit.ly, tinyurl)
3. Has IP URL
4. Domain age estimation
5. Suspicious keyword count (15 keywords)
6. Urgency word count
7. Request info count
8. Has unusual sender
9. Has html content
10. External link count
11. Attachment indicator
12. Caps ratio
13. Exclamation count
14. Special char ratio
15. Email length
16. Has @ symbol count
17. Has suspicious TLD
```

### **File Features (11 dimensions)**
```
1. File size
2. File extension length
3. Shannon entropy
4. Has PE header (MZ)
5. Has ELF header
6. Null byte ratio
7. Suspicious API strings count
8. Has ZIP header
9. Has executable opcodes
10. Average byte value
11. Magic number score
```

### **Shannon Entropy**
```
H(X) = -Œ£ P(xi) * log2(P(xi))
```
- Range: 0-8 bits
- High entropy (>7) = packed/encrypted
- Low entropy (<3) = plain text

**Implementation:**
```python
def _calculate_entropy(self, data: bytes) -> float:
    """Calculate Shannon entropy of file."""
    if len(data) == 0:
        return 0
    
    # Count byte frequencies
    byte_counts = np.bincount(np.frombuffer(data[:10000], dtype=np.uint8), minlength=256)
    probabilities = byte_counts / len(data[:10000])
    probabilities = probabilities[probabilities > 0]
    
    # Shannon entropy formula
    entropy = -np.sum(probabilities * np.log2(probabilities))
    return entropy
```

---

## üåç **3. NATURAL LANGUAGE PROCESSING**

### **Language Detection**
- **Algorithm**: Character n-gram frequency analysis
- **Libraries**: 
  - `langdetect` - Fast detection
  - `langid` - Backup detection
- **Supported**: Vietnamese, English, Chinese, Korean, Japanese, Thai, Spanish, French, German, Russian

### **Text Processing**
- **NLTK**: Tokenization, stopwords removal
- **Regex**: Pattern matching
- **Unicode**: Multi-language support

### **Keyword Detection**
- **Vietnamese**: 15 phishing keywords
- **English**: 15 phishing keywords
- **Chinese**: 10 phishing keywords
- **Multilingual risk multiplier**: 1.2x - 1.3x

**Implementation:**
```python
def detect_language(self, text: str) -> dict:
    """Detect language using langdetect and langid."""
    try:
        # Primary detection with langdetect
        from langdetect import detect_langs
        detected = detect_langs(text)
        
        primary_lang = detected[0].lang
        confidence = detected[0].prob
        
        all_languages = [d.lang for d in detected if d.prob > 0.1]
        
        # Multilingual check
        is_multilingual = len(all_languages) > 1
        
        # Risk multiplier based on language
        risk_multiplier = 1.0
        if primary_lang in ['vi', 'zh-cn', 'zh-tw', 'ko', 'th']:
            risk_multiplier = 1.2  # High-risk languages
        if is_multilingual:
            risk_multiplier = 1.3  # Multilingual = very suspicious
        
        return {
            'primary_language': primary_lang,
            'language_confidence': confidence,
            'all_languages': all_languages,
            'is_multilingual': is_multilingual,
            'risk_multiplier': risk_multiplier
        }
    except Exception as e:
        return {'primary_language': 'en', 'language_confidence': 0.5}
```

---

## üé® **4. USER INTERFACE TECHNOLOGIES**

### **Desktop App**
- **Framework**: PyQt6
- **Architecture**: Model-View-Controller (MVC)
- **Features**:
  - Dark theme UI
  - Real-time analysis
  - Multi-tab interface
  - Progress tracking
  - Result caching

### **Web App**
- **Framework**: Flask
- **Frontend**: HTML5, CSS3, JavaScript
- **API**: RESTful endpoints
- **Port**: 5000

---

## üîê **5. CRYPTOGRAPHY & HASHING**

### **Hash Algorithms**
- **MD5**: 128-bit hash (fast, collision-prone)
- **SHA-1**: 160-bit hash
- **SHA-256**: 256-bit hash (most secure)

**Implementation:**
```python
def get_file_hash(self, file_path: str) -> dict:
    """Calculate cryptographic hashes of file."""
    hashes = {}
    
    try:
        with open(file_path, 'rb') as f:
            data = f.read()
        
        hashes['md5'] = hashlib.md5(data).hexdigest()
        hashes['sha1'] = hashlib.sha1(data).hexdigest()
        hashes['sha256'] = hashlib.sha256(data).hexdigest()
    except Exception as e:
        print(f"Error calculating hash: {e}")
    
    return hashes
```

### **File Signature Detection**
```python
# Magic Bytes Detection
MALICIOUS_SIGNATURES = {
    b'MZ': 'PE_EXECUTABLE',      # Windows EXE/DLL
    b'\x7fELF': 'ELF_EXECUTABLE', # Linux executable
    b'PK\x03\x04': 'ZIP_ARCHIVE', # ZIP files
}

def _has_pe_header(self, data: bytes) -> int:
    """Check for PE (Windows executable) header."""
    return 1 if data.startswith(b'MZ') else 0

def _has_elf_header(self, data: bytes) -> int:
    """Check for ELF (Linux executable) header."""
    return 1 if data.startswith(b'\x7fELF') else 0

def _has_zip_header(self, data: bytes) -> int:
    """Check for ZIP archive header."""
    return 1 if data.startswith(b'PK\x03\x04') else 0
```

---

## üì¶ **6. DATA PROCESSING**

### **Libraries**
- **NumPy**: Array operations, numerical computing
- **Pandas**: CSV loading, data manipulation
- **Joblib**: Model serialization/deserialization

### **Data Formats**
- **CSV**: Training data storage
- **PKL**: Model persistence (pickle format)
- **JSON**: Configuration, API responses

---

## üèóÔ∏è **7. SOFTWARE ARCHITECTURE**

### **Design Patterns**
1. **Singleton**: Model caching (avoid reload)
2. **Factory**: Detector creation
3. **Observer**: Progress updates
4. **Strategy**: Multiple detection algorithms

### **Multi-threading**
- **QThread**: Background model training
- **Async I/O**: Non-blocking file operations

### **Caching Strategy**
- **Model cache**: Class-level singleton
- **Result cache**: In-memory dictionary
- **LRU policy**: Least Recently Used eviction

---

## üßÆ **8. STATISTICAL METHODS**

### **Cross-Validation**
- **K-Fold**: 5 splits
- **Stratified**: Balanced class distribution
- **Metric**: F1-score optimization

### **Evaluation Metrics**
```
Accuracy = (TP + TN) / (TP + TN + FP + FN)
Precision = TP / (TP + FP)
Recall = TP / (TP + FN)
F1-Score = 2 * (Precision * Recall) / (Precision + Recall)
ROC-AUC = Area Under ROC Curve
```

### **Risk Scoring**
```python
Risk Levels:
- Safe: Confidence ‚â• 70%
- Low Risk: 50-70%
- Uncertain: < 50%
- High: Phishing 60-80%
- Critical: Phishing ‚â• 80%
```

**Implementation:**
```python
def _calculate_risk_level(self, is_phishing: bool, confidence: float, 
                         language_multiplier: float = 1.0) -> str:
    """Calculate risk level based on prediction and confidence."""
    
    # Apply language risk multiplier
    adjusted_confidence = min(1.0, confidence * language_multiplier)
    
    if not is_phishing:
        if adjusted_confidence >= 0.7:
            return "Safe"
        elif adjusted_confidence >= 0.5:
            return "Low Risk"
        else:
            return "Uncertain"
    else:
        if adjusted_confidence >= 0.8:
            return "Critical"
        elif adjusted_confidence >= 0.6:
            return "High"
        else:
            return "Medium"
```

---

## üîç **9. PATTERN MATCHING ALGORITHMS**

### **Regex Patterns**
```python
# Pattern definitions
phishing_patterns = {
    'url_shortener': r'(bit\.ly|tinyurl|goo\.gl|short\.link)',
    'ip_address': r'http://\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}',
    'suspicious_domain': r'@[a-z0-9]*-[a-z0-9]*\.[a-z]+',
}

# Pattern matching implementation
def _extract_urls(self, text: str) -> list:
    """Extract all URLs from text using regex."""
    url_pattern = r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+'
    urls = re.findall(url_pattern, text.lower())
    return urls

def _has_shortener(self, urls: list) -> bool:
    """Check if any URL uses a shortening service."""
    shortener_pattern = re.compile(self.phishing_patterns['url_shortener'])
    return any(shortener_pattern.search(url) for url in urls)

def _has_ip_url(self, urls: list) -> bool:
    """Check if any URL uses IP address instead of domain."""
    ip_pattern = re.compile(self.phishing_patterns['ip_address'])
    return any(ip_pattern.search(url) for url in urls)
```

### **String Matching**
- **Boyer-Moore**: Fast substring search
- **KMP**: Pattern matching in suspicious strings

**Implementation:**
```python
# Suspicious API strings for malware detection
SUSPICIOUS_STRINGS = [
    b'CreateRemoteThread',
    b'WriteProcessMemory',
    b'SetWindowsHookEx',
    b'ShellExecute',
    b'WinExec',
    b'GetProcAddress',
    b'LoadLibrary',
]

def _count_suspicious_strings(self, data: bytes) -> int:
    """Count occurrences of suspicious API/function strings."""
    count = 0
    for suspicious_str in self.SUSPICIOUS_STRINGS:
        count += data.count(suspicious_str)
    return count
```

---

## üõ†Ô∏è **10. BUILD & DEPLOYMENT**

### **Packaging**
- **PyInstaller**: EXE compilation
- **onefile**: Single executable
- **windowed**: No console window
- **Size**: ~210-250 MB (with dependencies)

### **Dependencies**
```
Core ML: scikit-learn, numpy, scipy
NLP: nltk, langdetect, langid
GUI: PyQt6
Web: Flask
Utils: joblib, pathlib
```

### **Hidden Imports** (PyInstaller)
- pandas.plotting
- sklearn.ensemble
- sklearn.tree
- PyQt6.QtCore/QtGui/QtWidgets

---

## üìä **11. PERFORMANCE OPTIMIZATION**

### **Model Optimization**
- **Feature selection**: 17 ‚Üí 11 most important
- **Tree pruning**: max_depth limiting
- **Bootstrap sampling**: OOB score validation

### **Memory Management**
- **Lazy loading**: Load models on demand
- **Streaming**: Process large files in chunks
- **Max file size**: 100 MB limit

### **Speed Optimization**
- **Vectorization**: NumPy operations
- **Parallel processing**: n_jobs=-1 (all CPUs)
- **Model caching**: Singleton pattern

---

## üîÑ **12. DATA AUGMENTATION**

### **Feature Padding**
```python
# Auto-padding for different feature counts
if n_features < expected:
    padding = np.zeros(expected - n_features)
    features = np.hstack([features, padding])
```

**Full Implementation:**
```python
def predict(self, email_content: str, email_headers: dict = None) -> dict:
    """Analyze email and predict if it's phishing."""
    
    # Extract features
    features_dict = self._extract_feature_dict(email_content, email_headers)
    features = np.array(list(features_dict.values()), dtype=np.float32)
    
    # Auto-padding: if model expects more features, pad with zeros
    if hasattr(self, 'n_features') and len(features) < self.n_features:
        padding = np.zeros(self.n_features - len(features))
        features = np.hstack([features, padding])
        print(f"[INFO] Padded features from {len(features_dict)} to {self.n_features}")
    
    # Use ML model if available
    if self.use_ml_model:
        features_scaled = self.scaler.transform(features.reshape(1, -1))
        ml_prediction = int(self.model.predict(features_scaled)[0])
        proba = self.model.predict_proba(features_scaled)[0]
        ml_phishing_prob = float(proba[1])
        
        return {
            'is_phishing': bool(ml_prediction),
            'confidence': ml_phishing_prob,
            'features': features_dict
        }
    else:
        # Heuristic fallback
        heuristic_score = self._heuristic_score(features)
        return {
            'is_phishing': heuristic_score > 0.5,
            'confidence': heuristic_score,
            'features': features_dict
        }
```

### **Normalization**
- Auto-normalize different datasets
- Pad to max feature count
- Zero-fill missing features

**Multi-file training with different features:**
```python
# Check for consistent feature counts and normalize if needed
feature_counts = {}
for X_data, filename, n_features in X_list:
    if n_features not in feature_counts:
        feature_counts[n_features] = []
    feature_counts[n_features].append(filename)

# If multiple feature counts, normalize to max features
if len(feature_counts) > 1:
    max_features = max(feature_counts.keys())
    
    print(f"‚ö†Ô∏è Normalizing datasets with different feature counts:")
    for n_features, files in sorted(feature_counts.items()):
        print(f"  ‚Ä¢ {n_features} features: {', '.join(files)}")
    print(f"  ‚Üí Padding all to {max_features} features")
    
    # Normalize all arrays to max_features
    normalized_list = []
    for X_data, filename, n_features in X_list:
        if n_features < max_features:
            # Pad with zeros
            padding = np.zeros((X_data.shape[0], max_features - n_features))
            X_normalized = np.hstack([X_data, padding])
            normalized_list.append(X_normalized)
        else:
            normalized_list.append(X_data)
    
    X = np.vstack(normalized_list)
```

---

## üéØ **TECH STACK SUMMARY**

| Category | Technologies |
|----------|-------------|
| **Language** | Python 3.13 |
| **ML Framework** | scikit-learn 1.5+ |
| **NLP** | NLTK, langdetect, langid |
| **GUI** | PyQt6 |
| **Web** | Flask |
| **Data Science** | NumPy, Pandas, SciPy |
| **Build** | PyInstaller |
| **Version Control** | Git |
| **OS Support** | Windows, Linux, macOS |

---

## üìà **ALGORITHM COMPLEXITY**

| Operation | Time Complexity | Space Complexity |
|-----------|----------------|------------------|
| Feature Extraction (Email) | O(n) | O(1) |
| Feature Extraction (File) | O(n) | O(1) |
| Random Forest Training | O(m * n * log(n) * k) | O(m * k) |
| Random Forest Prediction | O(k * log(n)) | O(k) |
| Shannon Entropy | O(n) | O(256) |
| Language Detection | O(n) | O(1) |
| Hash Calculation | O(n) | O(1) |

Where:
- n = data size (email length / file size)
- m = number of samples
- k = number of trees (200)

---

## üöÄ **KEY INNOVATIONS**

1. ‚úÖ **Multilingual Phishing Detection** - Vietnamese/English/Chinese
2. ‚úÖ **Hybrid Detection** - ML + Heuristics fallback
3. ‚úÖ **Auto Feature Padding** - Handle different dataset formats
4. ‚úÖ **Risk Multipliers** - Language-based risk adjustment
5. ‚úÖ **Model Caching** - Singleton pattern for performance
6. ‚úÖ **Desktop + Web** - Dual interface support

---

**Version**: 2.1.0  
**Last Updated**: November 16, 2025  
**Total Lines of Code**: ~5,000+  
**Algorithms Used**: 12+ major algorithms  
**Technologies**: 15+ libraries & frameworks
